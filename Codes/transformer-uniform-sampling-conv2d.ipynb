{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5485767,"sourceType":"datasetVersion","datasetId":3166628}],"dockerImageVersionId":30461,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Import ","metadata":{}},{"cell_type":"code","source":"from skimage.io import imread\nfrom skimage.transform import resize\nimport numpy as np\nimport math\nimport os\nimport glob\nimport pandas as pd\nimport cv2\nimport gc\nimport numpy as np\nimport random\nimport imageio\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-04-28T04:22:40.330099Z","iopub.execute_input":"2023-04-28T04:22:40.330712Z","iopub.status.idle":"2023-04-28T04:22:59.289022Z","shell.execute_reply.started":"2023-04-28T04:22:40.330674Z","shell.execute_reply":"2023-04-28T04:22:59.287691Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Extract frames from video fucntion","metadata":{}},{"cell_type":"code","source":"def format_frames(frame, output_size):\n  \"\"\"\n    Pad and resize an image from a video.\n\n    Args:\n      frame: Image that needs to resized and padded. \n      output_size: Pixel size of the output frame image.\n\n    Return:\n      Formatted frame with padding of specified output size.\n  \"\"\"\n  frame = tf.image.convert_image_dtype(frame, tf.float32)\n  frame = tf.image.resize_with_pad(frame, *output_size)\n  return frame\n\ndef frames_from_video_file(video_path, n_frames, output_size = (120,180), frame_step = 5):\n  \"\"\"\n    Creates frames from each video file present for each category.\n\n    Args:\n      video_path: File path to the video.\n      n_frames: Number of frames to be created per video file.\n      output_size: Pixel size of the output frame image.\n\n    Return:\n      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n  \"\"\"\n  # Read each video frame by frame\n  result = []\n  src = cv2.VideoCapture(str(video_path))  \n\n  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n\n  need_length = 1 + (n_frames - 1) * frame_step\n\n  if need_length > video_length:\n    start = 0\n  else:\n    max_start = video_length - need_length\n    start = random.randint(0, 0)\n\n  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n  # ret is a boolean indicating whether read was successful, frame is the image itself\n  ret, frame = src.read()\n  result.append(format_frames(frame, output_size))\n\n  for _ in range(n_frames - 1):\n    for _ in range(frame_step):\n      ret, frame = src.read()\n    if ret:\n      frame = format_frames(frame, output_size)\n      result.append(frame)\n    else:\n      result.append(np.zeros_like(result[0]))\n  src.release()\n  result = np.array(result) #[..., [2, 1, 0]]\n\n  return result\n\n#def to_gif(images):\n # converted_images = np.clip(images * 255, 0, 255).astype(np.uint8)\n # imageio.mimsave('./animation.gif', converted_images, fps=10)\n # return embed.embed_file('./animation.gif')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T04:22:59.294734Z","iopub.execute_input":"2023-04-28T04:22:59.297602Z","iopub.status.idle":"2023-04-28T04:22:59.312797Z","shell.execute_reply.started":"2023-04-28T04:22:59.297560Z","shell.execute_reply":"2023-04-28T04:22:59.311574Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Get class names","metadata":{}},{"cell_type":"code","source":"class CFG:\n    epochs = 10\n    batch_size = 32\n    classes = os.listdir(\"/kaggle/input/ucf21-new/UCF101_new\")\n    #classes = [\"FloorGymnastics\",\"PullUps\"]","metadata":{"execution":{"iopub.status.busy":"2023-04-28T04:22:59.319577Z","iopub.execute_input":"2023-04-28T04:22:59.322240Z","iopub.status.idle":"2023-04-28T04:22:59.349754Z","shell.execute_reply.started":"2023-04-28T04:22:59.322192Z","shell.execute_reply":"2023-04-28T04:22:59.348706Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Get video paths ","metadata":{}},{"cell_type":"code","source":"file_paths = []\ntargets = []\ntarget_name = []\nfor i, cls in enumerate(CFG.classes):\n    sub_file_paths = glob.glob(f\"/kaggle/input/ucf21-new/UCF101_new/{cls}/**.avi\")\n    file_paths += sub_file_paths\n    targets += [i] * len(sub_file_paths)\n    target_name += [cls]","metadata":{"execution":{"iopub.status.busy":"2023-04-28T04:22:59.350760Z","iopub.execute_input":"2023-04-28T04:22:59.351088Z","iopub.status.idle":"2023-04-28T04:23:00.932789Z","shell.execute_reply.started":"2023-04-28T04:22:59.351051Z","shell.execute_reply":"2023-04-28T04:23:00.931771Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"len(file_paths)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T04:23:00.935727Z","iopub.execute_input":"2023-04-28T04:23:00.936418Z","iopub.status.idle":"2023-04-28T04:23:00.945588Z","shell.execute_reply.started":"2023-04-28T04:23:00.936377Z","shell.execute_reply":"2023-04-28T04:23:00.944482Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"4249"},"metadata":{}}]},{"cell_type":"code","source":"len(targets)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T04:23:00.946915Z","iopub.execute_input":"2023-04-28T04:23:00.947851Z","iopub.status.idle":"2023-04-28T04:23:00.956277Z","shell.execute_reply.started":"2023-04-28T04:23:00.947813Z","shell.execute_reply":"2023-04-28T04:23:00.954960Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"4249"},"metadata":{}}]},{"cell_type":"code","source":"label_dict = dict()\nfor i, video_label in enumerate(target_name):\n    label_dict[i] = video_label","metadata":{"execution":{"iopub.status.busy":"2023-04-28T04:23:02.169077Z","iopub.execute_input":"2023-04-28T04:23:02.170218Z","iopub.status.idle":"2023-04-28T04:23:02.176021Z","shell.execute_reply.started":"2023-04-28T04:23:02.170160Z","shell.execute_reply":"2023-04-28T04:23:02.174634Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Split the paths dataset to train and test","metadata":{}},{"cell_type":"code","source":"train_paths, test_paths, train_targets, test_targets = train_test_split(file_paths, targets, test_size=0.01, random_state=143)\nlen(train_paths), len(test_paths), len(train_targets), len(test_targets)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T04:23:05.267524Z","iopub.execute_input":"2023-04-28T04:23:05.268159Z","iopub.status.idle":"2023-04-28T04:23:05.296613Z","shell.execute_reply.started":"2023-04-28T04:23:05.268102Z","shell.execute_reply":"2023-04-28T04:23:05.295627Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(4206, 43, 4206, 43)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Split the train_paths dataset to train and val","metadata":{}},{"cell_type":"code","source":"train_paths, val_paths, train_targets, val_targets = train_test_split(train_paths, train_targets, test_size=0.2, random_state=143)\nlen(train_paths), len(val_paths), len(train_targets), len(val_targets)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T04:23:05.968813Z","iopub.execute_input":"2023-04-28T04:23:05.969554Z","iopub.status.idle":"2023-04-28T04:23:05.979441Z","shell.execute_reply.started":"2023-04-28T04:23:05.969515Z","shell.execute_reply":"2023-04-28T04:23:05.978054Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(3364, 842, 3364, 842)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Data loader","metadata":{}},{"cell_type":"code","source":"# using data loader to load images in batches\n# `x_set` is list of path to the images\n# `y_set` are the associated classes.\n\nclass train_DataLoader(tf.keras.utils.Sequence):\n\n    def __init__(self, x_set, y_set, batch_size):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return math.ceil(len(self.x) / self.batch_size)\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) *self.batch_size]\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n        #one hot encoding the labels data in our batch \n        frames_batch_x=[]\n        for video_path in batch_x:\n            frames_batch_x.append(frames_from_video_file(video_path, n_frames=10, output_size = (120,180)))          \n        #print(np.array(frames_batch_x).shape[-1])\n\n        return np.array(frames_batch_x), np.array(batch_y)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-28T04:23:17.566595Z","iopub.execute_input":"2023-04-28T04:23:17.567103Z","iopub.status.idle":"2023-04-28T04:23:17.582531Z","shell.execute_reply.started":"2023-04-28T04:23:17.567062Z","shell.execute_reply":"2023-04-28T04:23:17.580506Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# using data loader to load images in batches\n# `x_set` is list of path to the images\n# `y_set` are the associated classes.\n\nclass val_DataLoader(tf.keras.utils.Sequence):\n\n    def __init__(self, x_set, y_set, batch_size):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return math.ceil(len(self.x) / self.batch_size)\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) *self.batch_size]\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n        #one hot encoding the labels data in our batch \n        frames_batch_x=[]\n        for video_path in batch_x:\n            frames_batch_x.append(frames_from_video_file(video_path, n_frames=10, output_size = (120,180)))          \n        #print(np.array(frames_batch_x).shape[-1])\n        return np.array(frames_batch_x), np.array(batch_y)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-28T04:23:18.234531Z","iopub.execute_input":"2023-04-28T04:23:18.235055Z","iopub.status.idle":"2023-04-28T04:23:18.249046Z","shell.execute_reply.started":"2023-04-28T04:23:18.235011Z","shell.execute_reply":"2023-04-28T04:23:18.247932Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"### CNN","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout,GlobalAveragePooling2D","metadata":{"execution":{"iopub.status.busy":"2023-04-26T06:46:18.458105Z","iopub.execute_input":"2023-04-26T06:46:18.458835Z","iopub.status.idle":"2023-04-26T06:46:18.468529Z","shell.execute_reply.started":"2023-04-26T06:46:18.458791Z","shell.execute_reply":"2023-04-26T06:46:18.467367Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Loading VGG16 model\nbase_model = VGG16(weights = \"imagenet\", include_top = False, input_shape = (120,180,3))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T06:46:19.077210Z","iopub.execute_input":"2023-04-26T06:46:19.078611Z","iopub.status.idle":"2023-04-26T06:46:25.128669Z","shell.execute_reply.started":"2023-04-26T06:46:19.078567Z","shell.execute_reply":"2023-04-26T06:46:25.127477Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n58889256/58889256 [==============================] - 2s 0us/step\n","output_type":"stream"}]},{"cell_type":"code","source":"cnn_model = Sequential()\nfor layer in base_model.layers:\n    cnn_model.add(layer)\ncnn_model.add(Flatten())","metadata":{"execution":{"iopub.status.busy":"2023-04-26T06:46:25.130515Z","iopub.execute_input":"2023-04-26T06:46:25.131125Z","iopub.status.idle":"2023-04-26T06:46:25.230446Z","shell.execute_reply.started":"2023-04-26T06:46:25.131088Z","shell.execute_reply":"2023-04-26T06:46:25.229503Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Transformer model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef transformer_block(inputs, num_heads, ff_dim, dropout_rate):\n    # Multi-Head Attention\n    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1])(inputs, inputs)\n    attn_output = layers.Dropout(dropout_rate)(attn_output)\n    out1 = layers.LayerNormalization(epsilon=1.001e-5)(inputs + attn_output)\n\n    # Feed Forward layer\n    ffn = tf.keras.Sequential(\n        [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(inputs.shape[-1])]\n    )\n    ffn_output = ffn(out1)\n    ffn_output = layers.Dropout(dropout_rate)(ffn_output)\n    out2 = layers.LayerNormalization(epsilon=1.001e-5)(out1 + ffn_output)\n    return out2\n","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:45:24.528699Z","iopub.execute_input":"2023-04-26T17:45:24.529389Z","iopub.status.idle":"2023-04-26T17:45:24.538219Z","shell.execute_reply.started":"2023-04-26T17:45:24.529350Z","shell.execute_reply":"2023-04-26T17:45:24.537149Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\n\ndef get_transformer_model(num_frames, height, width, channels, num_classes):\n    inputs = layers.Input(shape=(num_frames, height, width, channels))\n    x = layers.TimeDistributed(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))(inputs)\n    x = layers.TimeDistributed(layers.MaxPooling2D((2, 2), strides=(3, 3)))(x)\n    x = layers.TimeDistributed(layers.Conv2D(128, (3, 3), activation='relu', padding='same'))(x)\n    x = layers.TimeDistributed(layers.MaxPooling2D((2, 2), strides=(3, 3)))(x)\n    x = layers.TimeDistributed(layers.Conv2D(256, (3, 3), activation='relu', padding='same'))(x)\n    x = layers.TimeDistributed(layers.MaxPooling2D((2, 2), strides=(3, 3)))(x)\n    x = layers.TimeDistributed(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))(x)\n    x = layers.TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2)))(x)\n    x = layers.TimeDistributed(layers.Conv2D(512, (3, 3), activation='relu', padding='same'))(x)\n    x = layers.TimeDistributed(layers.MaxPooling2D((2, 2), strides=(2, 2)))(x)\n    \n    x = layers.TimeDistributed(layers.Flatten())(x)\n    x = transformer_block(inputs=x, num_heads=8, ff_dim=512, dropout_rate=0.1)\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dense(1024, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(512, activation='relu')(x)\n    x = layers.Dropout(0.5)(x)\n    outputs = layers.Dense(num_classes, activation='softmax')(x)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:45:26.943691Z","iopub.execute_input":"2023-04-26T17:45:26.944401Z","iopub.status.idle":"2023-04-26T17:45:26.957653Z","shell.execute_reply.started":"2023-04-26T17:45:26.944363Z","shell.execute_reply":"2023-04-26T17:45:26.956408Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"a=tf.keras.optimizers.Adam(\n    learning_rate=0.0001)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:45:30.789114Z","iopub.execute_input":"2023-04-26T17:45:30.789475Z","iopub.status.idle":"2023-04-26T17:45:32.959361Z","shell.execute_reply.started":"2023-04-26T17:45:30.789436Z","shell.execute_reply":"2023-04-26T17:45:32.958322Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# compile the model\n\nmodel = get_transformer_model(num_frames=10, height=120, width=180, channels=3, num_classes=21)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=a, metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:45:41.032280Z","iopub.execute_input":"2023-04-26T17:45:41.033028Z","iopub.status.idle":"2023-04-26T17:45:41.462311Z","shell.execute_reply.started":"2023-04-26T17:45:41.032986Z","shell.execute_reply":"2023-04-26T17:45:41.461251Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"checkpoint = tf.keras.callbacks.ModelCheckpoint(\n    \"model.h5\", \n    monitor=\"val_accuracy\",\n    mode=\"max\",\n    save_best_only=True, \n    restore_best_weights=True\n)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:45:43.658224Z","iopub.execute_input":"2023-04-26T17:45:43.658631Z","iopub.status.idle":"2023-04-26T17:45:43.665030Z","shell.execute_reply.started":"2023-04-26T17:45:43.658595Z","shell.execute_reply":"2023-04-26T17:45:43.663493Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# fit the model\nhistory = model.fit(train_DataLoader(train_paths, train_targets,batch_size=4), \n                    validation_data=val_DataLoader(val_paths, val_targets,batch_size=4), \n                    epochs=5,callbacks=[checkpoint])","metadata":{"execution":{"iopub.status.busy":"2023-04-26T10:51:00.287878Z","iopub.execute_input":"2023-04-26T10:51:00.289166Z","iopub.status.idle":"2023-04-26T11:25:28.348932Z","shell.execute_reply.started":"2023-04-26T10:51:00.289119Z","shell.execute_reply":"2023-04-26T11:25:28.347832Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Epoch 1/5\n841/841 [==============================] - 441s 519ms/step - loss: 2.8781 - accuracy: 0.2598 - val_loss: 2.5478 - val_accuracy: 0.3337\nEpoch 2/5\n841/841 [==============================] - 395s 470ms/step - loss: 2.4609 - accuracy: 0.3160 - val_loss: 2.2847 - val_accuracy: 0.3895\nEpoch 3/5\n841/841 [==============================] - 394s 468ms/step - loss: 2.3009 - accuracy: 0.3413 - val_loss: 2.2291 - val_accuracy: 0.3717\nEpoch 4/5\n841/841 [==============================] - 396s 471ms/step - loss: 2.1626 - accuracy: 0.3618 - val_loss: 2.0336 - val_accuracy: 0.3907\nEpoch 5/5\n841/841 [==============================] - 392s 466ms/step - loss: 2.0391 - accuracy: 0.3784 - val_loss: 1.9101 - val_accuracy: 0.4276\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save(\"/kaggle/working/transformer_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:37:49.268244Z","iopub.execute_input":"2023-04-26T11:37:49.268636Z","iopub.status.idle":"2023-04-26T11:37:49.554833Z","shell.execute_reply.started":"2023-04-26T11:37:49.268599Z","shell.execute_reply":"2023-04-26T11:37:49.553676Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.load_model(\"/kaggle/input/transformer-model-1/transformer_model.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:52:43.423390Z","iopub.execute_input":"2023-04-26T11:52:43.424028Z","iopub.status.idle":"2023-04-26T11:52:46.065176Z","shell.execute_reply.started":"2023-04-26T11:52:43.423989Z","shell.execute_reply":"2023-04-26T11:52:46.064096Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# fit the model\nhistory = model.fit(train_DataLoader(train_paths, train_targets,batch_size=4), \n                    validation_data=val_DataLoader(val_paths, val_targets,batch_size=4), \n                    epochs=3,callbacks=[checkpoint])","metadata":{"execution":{"iopub.status.busy":"2023-04-26T11:53:14.589155Z","iopub.execute_input":"2023-04-26T11:53:14.589868Z","iopub.status.idle":"2023-04-26T12:12:53.155127Z","shell.execute_reply.started":"2023-04-26T11:53:14.589827Z","shell.execute_reply":"2023-04-26T12:12:53.154097Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"Epoch 1/3\n841/841 [==============================] - 398s 468ms/step - loss: 1.9206 - accuracy: 0.4010 - val_loss: 1.8804 - val_accuracy: 0.4276\nEpoch 2/3\n841/841 [==============================] - 390s 463ms/step - loss: 1.8060 - accuracy: 0.4510 - val_loss: 1.6800 - val_accuracy: 0.5083\nEpoch 3/3\n841/841 [==============================] - 390s 464ms/step - loss: 1.6355 - accuracy: 0.4848 - val_loss: 1.5197 - val_accuracy: 0.5154\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save(\"/kaggle/working/transformer_model_2.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T12:29:37.548117Z","iopub.execute_input":"2023-04-26T12:29:37.548899Z","iopub.status.idle":"2023-04-26T12:29:37.841763Z","shell.execute_reply.started":"2023-04-26T12:29:37.548859Z","shell.execute_reply":"2023-04-26T12:29:37.840725Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.load_model(\"/kaggle/input/transformer-model-2/transformer_model_2.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T13:03:12.340749Z","iopub.execute_input":"2023-04-26T13:03:12.341559Z","iopub.status.idle":"2023-04-26T13:03:14.554027Z","shell.execute_reply.started":"2023-04-26T13:03:12.341517Z","shell.execute_reply":"2023-04-26T13:03:14.552779Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# fit the model\nhistory = model.fit(train_DataLoader(train_paths, train_targets,batch_size=4), \n                    validation_data=val_DataLoader(val_paths, val_targets,batch_size=4), \n                    epochs=3,callbacks=[checkpoint])","metadata":{"execution":{"iopub.status.busy":"2023-04-26T13:03:35.945315Z","iopub.execute_input":"2023-04-26T13:03:35.945872Z","iopub.status.idle":"2023-04-26T13:23:08.218050Z","shell.execute_reply.started":"2023-04-26T13:03:35.945827Z","shell.execute_reply":"2023-04-26T13:23:08.217018Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Epoch 1/3\n841/841 [==============================] - 394s 463ms/step - loss: 1.4873 - accuracy: 0.5288 - val_loss: 1.3440 - val_accuracy: 0.5701\nEpoch 2/3\n841/841 [==============================] - 389s 462ms/step - loss: 1.3614 - accuracy: 0.5633 - val_loss: 1.2343 - val_accuracy: 0.6188\nEpoch 3/3\n841/841 [==============================] - 389s 463ms/step - loss: 1.2163 - accuracy: 0.6118 - val_loss: 1.2113 - val_accuracy: 0.6544\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save(\"/kaggle/working/transformer_model_3.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T13:33:46.346726Z","iopub.execute_input":"2023-04-26T13:33:46.347718Z","iopub.status.idle":"2023-04-26T13:33:46.638793Z","shell.execute_reply.started":"2023-04-26T13:33:46.347662Z","shell.execute_reply":"2023-04-26T13:33:46.637737Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.load_model(\"/kaggle/input/transformer-model-3/transformer_model_3.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T14:03:09.541825Z","iopub.execute_input":"2023-04-26T14:03:09.542212Z","iopub.status.idle":"2023-04-26T14:03:12.309707Z","shell.execute_reply.started":"2023-04-26T14:03:09.542175Z","shell.execute_reply":"2023-04-26T14:03:12.308646Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# fit the model\nhistory = model.fit(train_DataLoader(train_paths, train_targets,batch_size=4), \n                    validation_data=val_DataLoader(val_paths, val_targets,batch_size=4), \n                    epochs=4,callbacks=[checkpoint])","metadata":{"execution":{"iopub.status.busy":"2023-04-26T14:03:30.517231Z","iopub.execute_input":"2023-04-26T14:03:30.517628Z","iopub.status.idle":"2023-04-26T14:30:18.723319Z","shell.execute_reply.started":"2023-04-26T14:03:30.517591Z","shell.execute_reply":"2023-04-26T14:30:18.721785Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Epoch 1/4\n841/841 [==============================] - 392s 461ms/step - loss: 1.0638 - accuracy: 0.6587 - val_loss: 1.1424 - val_accuracy: 0.6639\nEpoch 2/4\n841/841 [==============================] - 389s 462ms/step - loss: 0.9341 - accuracy: 0.6825 - val_loss: 1.0303 - val_accuracy: 0.6865\nEpoch 3/4\n841/841 [==============================] - 385s 458ms/step - loss: 0.8445 - accuracy: 0.7363 - val_loss: 1.0240 - val_accuracy: 0.7577\nEpoch 4/4\n841/841 [==============================] - 382s 455ms/step - loss: 0.7328 - accuracy: 0.7545 - val_loss: 0.7937 - val_accuracy: 0.7791\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save(\"/kaggle/working/transformer_model_4.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T14:33:48.981967Z","iopub.execute_input":"2023-04-26T14:33:48.982716Z","iopub.status.idle":"2023-04-26T14:33:49.286474Z","shell.execute_reply.started":"2023-04-26T14:33:48.982674Z","shell.execute_reply":"2023-04-26T14:33:49.285458Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.load_model(\"/kaggle/input/transformer-model-4/transformer_model_4.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T14:53:16.611392Z","iopub.execute_input":"2023-04-26T14:53:16.612050Z","iopub.status.idle":"2023-04-26T14:53:18.950328Z","shell.execute_reply.started":"2023-04-26T14:53:16.612007Z","shell.execute_reply":"2023-04-26T14:53:18.949276Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# fit the model\nhistory = model.fit(train_DataLoader(train_paths, train_targets,batch_size=4), \n                    validation_data=val_DataLoader(val_paths, val_targets,batch_size=4), \n                    epochs=5,callbacks=[checkpoint])","metadata":{"execution":{"iopub.status.busy":"2023-04-26T14:53:40.409983Z","iopub.execute_input":"2023-04-26T14:53:40.411020Z","iopub.status.idle":"2023-04-26T15:25:55.633344Z","shell.execute_reply.started":"2023-04-26T14:53:40.410968Z","shell.execute_reply":"2023-04-26T15:25:55.632320Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"Epoch 1/5\n841/841 [==============================] - 392s 460ms/step - loss: 0.6317 - accuracy: 0.7973 - val_loss: 0.8010 - val_accuracy: 0.7791\nEpoch 2/5\n841/841 [==============================] - 388s 461ms/step - loss: 0.5206 - accuracy: 0.8315 - val_loss: 0.7685 - val_accuracy: 0.7886\nEpoch 3/5\n841/841 [==============================] - 385s 458ms/step - loss: 0.5010 - accuracy: 0.8395 - val_loss: 0.8576 - val_accuracy: 0.8040\nEpoch 4/5\n841/841 [==============================] - 385s 458ms/step - loss: 0.4226 - accuracy: 0.8716 - val_loss: 0.9481 - val_accuracy: 0.7910\nEpoch 5/5\n841/841 [==============================] - 384s 457ms/step - loss: 0.3500 - accuracy: 0.8980 - val_loss: 0.7327 - val_accuracy: 0.8183\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save(\"/kaggle/working/transformer_model_5.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T15:27:11.583753Z","iopub.execute_input":"2023-04-26T15:27:11.584135Z","iopub.status.idle":"2023-04-26T15:27:11.913404Z","shell.execute_reply.started":"2023-04-26T15:27:11.584102Z","shell.execute_reply":"2023-04-26T15:27:11.912375Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"model = tf.keras.models.load_model(\"/kaggle/input/transformer-final/transformer_conv2D_final.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:45:53.028407Z","iopub.execute_input":"2023-04-26T17:45:53.029533Z","iopub.status.idle":"2023-04-26T17:45:55.680062Z","shell.execute_reply.started":"2023-04-26T17:45:53.029489Z","shell.execute_reply":"2023-04-26T17:45:55.679023Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# fit the model\nhistory = model.fit(train_DataLoader(train_paths, train_targets,batch_size=4), \n                    validation_data=val_DataLoader(val_paths, val_targets,batch_size=4), \n                    epochs=5,callbacks=[checkpoint])","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:46:23.069627Z","iopub.execute_input":"2023-04-26T17:46:23.070465Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5\n841/841 [==============================] - 435s 504ms/step - loss: 0.3080 - accuracy: 0.9043 - val_loss: 0.6511 - val_accuracy: 0.8575\nEpoch 2/5\n841/841 [==============================] - 405s 481ms/step - loss: 0.2594 - accuracy: 0.9242 - val_loss: 0.6609 - val_accuracy: 0.8587\nEpoch 3/5\n841/841 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.9227","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit the model\nhistory = model.fit(train_DataLoader(train_paths, train_targets,batch_size=4), \n                    validation_data=val_DataLoader(val_paths, val_targets,batch_size=4), \n                    epochs=5,callbacks=[checkpoint])","metadata":{"execution":{"iopub.status.busy":"2023-04-26T07:03:08.975381Z","iopub.execute_input":"2023-04-26T07:03:08.975824Z","iopub.status.idle":"2023-04-26T07:08:40.583989Z","shell.execute_reply.started":"2023-04-26T07:03:08.975775Z","shell.execute_reply":"2023-04-26T07:08:40.582107Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Epoch 1/5\n468/841 [===============>..............] - ETA: 4:19 - loss: 2.9330 - accuracy: 0.2484","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/846183058.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(train_DataLoader(train_paths, train_targets,batch_size=4), \n\u001b[1;32m      3\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_DataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                     epochs=5,callbacks=[checkpoint])\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1648\u001b[0m                         ):\n\u001b[1;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1650\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1651\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m    134\u001b[0m     return concrete_function._call_flat(\n\u001b[0;32m--> 135\u001b[0;31m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    381\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 53\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# fit the model\nhistory = model.fit(train_DataLoader(train_paths, train_targets,batch_size=4), \n                    validation_data=val_DataLoader(val_paths, val_targets,batch_size=4), \n                    epochs=10,callbacks=[checkpoint])\n","metadata":{"execution":{"iopub.status.busy":"2023-04-26T07:02:55.712222Z","iopub.status.idle":"2023-04-26T07:02:55.712817Z","shell.execute_reply.started":"2023-04-26T07:02:55.712513Z","shell.execute_reply":"2023-04-26T07:02:55.712541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fit the model\nhistory = model.fit(train_DataLoader(train_paths, train_targets,batch_size=4), \n                    validation_data=val_DataLoader(val_paths, val_targets,batch_size=4), \n                    epochs=10,callbacks=[checkpoint])\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T20:22:45.894480Z","iopub.execute_input":"2023-04-25T20:22:45.894941Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/10\n841/841 [==============================] - 823s 972ms/step - loss: 2.9274 - accuracy: 0.2649 - val_loss: 2.5683 - val_accuracy: 0.3207\nEpoch 2/10\n841/841 [==============================] - 715s 850ms/step - loss: 2.5603 - accuracy: 0.3032 - val_loss: 2.3676 - val_accuracy: 0.3029\nEpoch 3/10\n 86/841 [==>...........................] - ETA: 8:44 - loss: 2.4679 - accuracy: 0.2791","output_type":"stream"}]},{"cell_type":"code","source":"#accuracy plot\nplt.figure(figsize = (8,5))\nx = np.arange(0,10,1)\nplt.plot(x,history.history[\"accuracy\"],label = \"training accuracy\",c = 'blue')\nplt.plot(x,history.history[\"val_accuracy\"],label = \"validation accuracy\",c = 'red')\nplt.legend()\nplt.title(\"Accuracy vs epochs (Transformer model)\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"Accuracy\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#accuracy plot\nplt.figure(figsize = (8,5))\nx = np.arange(0,10,1)\nplt.plot(x,history.history[\"loss\"],label = \"training loss\",c = 'blue')\nplt.plot(x,history.history[\"val_loss\"],label = \"validation loss\",c = 'red')\nplt.legend()\nplt.title(\"Loss vs epochs (Transformer model)\")\nplt.xlabel(\"epochs\")\nplt.ylabel(\"Loss\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"test_videos_path = test_paths\nnum = np.random.randint(0,len(test_videos_path),1)\ntest_file_path = test_videos_path[num[0]]\ntest_file_path","metadata":{"execution":{"iopub.status.busy":"2023-04-26T15:46:48.311621Z","iopub.execute_input":"2023-04-26T15:46:48.312363Z","iopub.status.idle":"2023-04-26T15:46:48.322380Z","shell.execute_reply.started":"2023-04-26T15:46:48.312322Z","shell.execute_reply":"2023-04-26T15:46:48.321096Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"'/kaggle/input/ucf21-new/UCF101_new/PlayingCricket/v_CricketShot_g11_c03.avi'"},"metadata":{}}]},{"cell_type":"code","source":"features = []\nfeatures.append(frames_from_video_file(test_file_path, n_frames = 10))\nfeatures = np.array(features)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-26T15:46:53.668167Z","iopub.execute_input":"2023-04-26T15:46:53.668651Z","iopub.status.idle":"2023-04-26T15:46:53.773048Z","shell.execute_reply.started":"2023-04-26T15:46:53.668614Z","shell.execute_reply":"2023-04-26T15:46:53.772013Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"predicted_class = model.predict(np.array(features))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T15:46:56.185184Z","iopub.execute_input":"2023-04-26T15:46:56.185573Z","iopub.status.idle":"2023-04-26T15:46:56.667400Z","shell.execute_reply.started":"2023-04-26T15:46:56.185538Z","shell.execute_reply":"2023-04-26T15:46:56.666447Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 420ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Getting indices of N = 3 maximum values\nindex = np.argsort(predicted_class[0])[::-1][:3]\n#print(\"Indices:\",index)\n\n# Getting N maximum values\npercentage_class = predicted_class[0][index]\n#print(\"Values:\",percentage_class)\n\nnum = -1\nfor i in index:\n    num+=1\n    a = label_dict[i]\n    b = percentage_class[num]*100\n    print(f\"{a} : {b:.3f} percent\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T15:46:59.180234Z","iopub.execute_input":"2023-04-26T15:46:59.180953Z","iopub.status.idle":"2023-04-26T15:46:59.188383Z","shell.execute_reply.started":"2023-04-26T15:46:59.180914Z","shell.execute_reply":"2023-04-26T15:46:59.187112Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"PlayingCricket : 99.942 percent\nApplyingMakeup : 0.042 percent\nSkyDiving : 0.007 percent\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save(\"/kaggle/working/transformer_conv2D_final.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T15:48:41.212581Z","iopub.execute_input":"2023-04-26T15:48:41.214134Z","iopub.status.idle":"2023-04-26T15:48:41.583738Z","shell.execute_reply.started":"2023-04-26T15:48:41.214082Z","shell.execute_reply":"2023-04-26T15:48:41.582714Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test video","metadata":{}},{"cell_type":"code","source":"model = tf.keras.models.load_model(\"/kaggle/input/transformer-final/transformer_conv2D_final.h5\")","metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:01:00.583530Z","iopub.execute_input":"2023-04-27T10:01:00.583883Z","iopub.status.idle":"2023-04-27T10:01:01.256411Z","shell.execute_reply.started":"2023-04-27T10:01:00.583852Z","shell.execute_reply":"2023-04-27T10:01:01.255357Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"test_videos_path = r\"/kaggle/input/test-video-3/test3.mp4\"","metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:01:01.258623Z","iopub.execute_input":"2023-04-27T10:01:01.258999Z","iopub.status.idle":"2023-04-27T10:01:01.263763Z","shell.execute_reply.started":"2023-04-27T10:01:01.258950Z","shell.execute_reply":"2023-04-27T10:01:01.262691Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"features = []\nfeatures.append(frames_from_video_file(test_videos_path, n_frames = 10))\nfeatures = np.array(features)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:01:18.167548Z","iopub.execute_input":"2023-04-27T10:01:18.168252Z","iopub.status.idle":"2023-04-27T10:01:18.338620Z","shell.execute_reply.started":"2023-04-27T10:01:18.168210Z","shell.execute_reply":"2023-04-27T10:01:18.337583Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"predicted_class = model.predict(np.array(features))","metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:01:19.480677Z","iopub.execute_input":"2023-04-27T10:01:19.481135Z","iopub.status.idle":"2023-04-27T10:01:19.964618Z","shell.execute_reply.started":"2023-04-27T10:01:19.481091Z","shell.execute_reply":"2023-04-27T10:01:19.963651Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 420ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Getting indices of N = 3 maximum values\nindex = np.argsort(predicted_class[0])[::-1][:5]\n#print(\"Indices:\",index)\n\n# Getting N maximum values\npercentage_class = predicted_class[0][index]\n#print(\"Values:\",percentage_class)\n\nnum = -1\nfor i in index:\n    num+=1\n    a = label_dict[i]\n    b = percentage_class[num]*100\n    print(f\"{a} : {b:.3f} percent\")","metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:01:22.375652Z","iopub.execute_input":"2023-04-27T10:01:22.376014Z","iopub.status.idle":"2023-04-27T10:01:22.383797Z","shell.execute_reply.started":"2023-04-27T10:01:22.375981Z","shell.execute_reply":"2023-04-27T10:01:22.382454Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"ApplyingMakeup : 36.324 percent\nPullUps : 34.044 percent\nSalsaSpin : 9.698 percent\nPlayingMusicalInstrument : 5.971 percent\nSkyDiving : 3.804 percent\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted_class = model.predict(np.array(features))","metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:01:57.657362Z","iopub.execute_input":"2023-04-27T10:01:57.658029Z","iopub.status.idle":"2023-04-27T10:01:57.725921Z","shell.execute_reply.started":"2023-04-27T10:01:57.657988Z","shell.execute_reply":"2023-04-27T10:01:57.725115Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"1/1 [==============================] - 0s 25ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Getting indices of N = 3 maximum values\nindex = np.argsort(predicted_class[0])[::-1][:5]\n#print(\"Indices:\",index)\n\n# Getting N maximum values\npercentage_class = predicted_class[0][index]\n#print(\"Values:\",percentage_class)\n\nnum = -1\nfor i in index:\n    num+=1\n    a = label_dict[i]\n    b = percentage_class[num]*100\n    print(f\"{a} : {b:.3f} percent\")","metadata":{"execution":{"iopub.status.busy":"2023-04-27T10:01:58.401159Z","iopub.execute_input":"2023-04-27T10:01:58.402089Z","iopub.status.idle":"2023-04-27T10:01:58.409486Z","shell.execute_reply.started":"2023-04-27T10:01:58.402013Z","shell.execute_reply":"2023-04-27T10:01:58.408311Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"ApplyingMakeup : 36.324 percent\nPullUps : 34.044 percent\nSalsaSpin : 9.698 percent\nPlayingMusicalInstrument : 5.971 percent\nSkyDiving : 3.804 percent\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}